{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial, reduce\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = vgg16(False).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutipleKeysDict(OrderedDict):\n",
    "    \"\"\"\n",
    "    Allow to get values from multiple keys. Example:\n",
    "    \n",
    "    ```python\n",
    "    d = MutipleKeysDict({ 'a' : 1, 'b' : 2, 'c' : 3})\n",
    "    d[['a', 'b']]\n",
    "    # out [1,2]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __getitem__(self, keys):\n",
    "        \n",
    "        if type(keys) is list:\n",
    "            res = [dict.__getitem__(self, key) for key in keys]\n",
    "        else: res = super().__getitem__(keys)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleStorage():\n",
    "    def __init__(self, where2layers, debug=False):\n",
    "        self.where2layers = where2layers\n",
    "        self.where = list(self.names)[0]\n",
    "        self.state = self._state\n",
    "        self.unsubcribe = []\n",
    "        self.debug = debug\n",
    "    \n",
    "    @property\n",
    "    def _state(self):\n",
    "        return MutipleKeysDict({ \n",
    "            k : MutipleKeysDict() if type(self.where2layers) == dict else [] \n",
    "            for k in self.names \n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def names(self):\n",
    "        names = []\n",
    "        if type(self.where2layers) == dict:\n",
    "            names = self.where2layers.keys()\n",
    "        elif type(self.where2layers) is list:\n",
    "            names = self.where2layers\n",
    "        return names\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        \"\"\"\n",
    "        Flat all the layers in the same array\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        if type(self.where2layers) == dict:\n",
    "            layers = reduce(lambda a, b: a + b, self.where2layers.values())\n",
    "        elif type(self.where2layers) is list:\n",
    "            layers = self.where2layers\n",
    "        return layers \n",
    "    \n",
    "    def register_hooks(self, how='forward'):\n",
    "        \"\"\"\n",
    "        Loop in all the layers and register a hook. There is ONLY one hook per layer to improve\n",
    "        performance.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # create a hash of a layer as an identifier, this is unique\n",
    "#             name = f\"{type(layer).__name__.lower()}-{hash(layer)}\"\n",
    "            if how == 'forward':\n",
    "                self.unsubcribe.append(layer.register_forward_hook(partial(self.hook, name=layer)))\n",
    "            elif how == 'backward':\n",
    "                self.unsubcribe.append(layer.register_backward_hook(partial(self.hook, name=layer)))\n",
    "            else:\n",
    "                raise ValueError(\"type must be 'forward' or 'backward'\")\n",
    "            if self.debug: print(f\"[INFO] {how} hook registered to {layer}\")\n",
    "        \n",
    "    def hook(self, m, i, o, name):\n",
    "        if self.debug: print(f\"{m} called\")\n",
    "            \n",
    "        if type(self.where2layers) == dict:\n",
    "    #       store only the outputs from the correct layers defined in self.where2layers\n",
    "            if m in self.where2layers[self.where]: self.state[self.where][name] = o\n",
    "        if type(self.where2layers) is list:\n",
    "            self.state[name].append(o) \n",
    "            \n",
    "    def clear(self):\n",
    "        if self.debug: print('[INFO] clear')\n",
    "        [un.remove() for un in self.unsubcribe]\n",
    "\n",
    "    def __call__(self, where=None):\n",
    "        if where is not None:\n",
    "            if where not in self.keys(): raise(f\"we cannot find any layers with key {where}\")\n",
    "            self.where = where\n",
    "        \n",
    "    def __repr__(self):\n",
    "        items = lambda x: x.items() if type(x) == MutipleKeysDict else enumerate(x)\n",
    "        return str({k: [{i : e.shape for i, e in items(v)}] for k, v in self.state.items()})    \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.state[key]\n",
    "    \n",
    "    def keys(self):\n",
    "        return self.state.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store input for multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys([Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))])\n"
     ]
    }
   ],
   "source": [
    "class ForwardModuleStorage(ModuleStorage):\n",
    "    def __init__(self, module, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.module = module\n",
    "        self.register_hooks(how='forward')\n",
    "        \n",
    "        \n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        super().__call__(*args, **kwargs)\n",
    "        if type(x) != list: x = [x]\n",
    "        [self.module(_x) for _x in x]\n",
    "        \n",
    "storage = ForwardModuleStorage(cnn, {'style' : [cnn.features[5]], 'content' : [cnn.features[5], cnn.features[10]]})\n",
    "storage(torch.rand(1,3,100,100).to(device), 'style')\n",
    "storage(torch.rand(1,3,100,100).to(device), 'content')\n",
    "\n",
    "pprint(storage['style'].keys())\n",
    "\n",
    "storage.clear()\n",
    "\n",
    "del storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store multiple inputs for same layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0639, 0.1510,  ..., 0.1072, 0.2296, 0.2635],\n",
      "          [0.0000, 0.2039, 0.0981,  ..., 0.0552, 0.1525, 0.2407],\n",
      "          [0.0000, 0.0676, 0.1154,  ..., 0.0868, 0.2180, 0.1837],\n",
      "          ...,\n",
      "          [0.0000, 0.0719, 0.1673,  ..., 0.1239, 0.1000, 0.2005],\n",
      "          [0.0000, 0.1108, 0.0981,  ..., 0.0817, 0.1170, 0.2060],\n",
      "          [0.0000, 0.0204, 0.0000,  ..., 0.0000, 0.0400, 0.0833]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0962, 0.1169,  ..., 0.0994, 0.0512, 0.0926]],\n",
      "\n",
      "         [[0.2010, 0.1628, 0.2299,  ..., 0.2754, 0.1224, 0.0969],\n",
      "          [0.2261, 0.0469, 0.0210,  ..., 0.0000, 0.0000, 0.0007],\n",
      "          [0.1354, 0.0505, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1594, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0739, 0.0174, 0.0015,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0710, 0.1394,  ..., 0.1151, 0.1851, 0.1788],\n",
      "          [0.0335, 0.2938, 0.2569,  ..., 0.2509, 0.2469, 0.3626],\n",
      "          [0.0200, 0.2005, 0.2231,  ..., 0.1421, 0.2382, 0.2971],\n",
      "          ...,\n",
      "          [0.1300, 0.2280, 0.2797,  ..., 0.2157, 0.2145, 0.3134],\n",
      "          [0.1328, 0.2739, 0.2620,  ..., 0.1939, 0.3083, 0.3529],\n",
      "          [0.0000, 0.1032, 0.1180,  ..., 0.1404, 0.1823, 0.2083]]]],\n",
      "       grad_fn=<ReluBackward1>),\n",
      " tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.1636e-01, 8.4750e-02,  ..., 1.3383e-01,\n",
      "           1.2086e-01, 2.4677e-01],\n",
      "          [0.0000e+00, 1.0689e-01, 6.3760e-02,  ..., 2.0695e-01,\n",
      "           8.9843e-02, 1.7956e-01],\n",
      "          [0.0000e+00, 7.2678e-02, 9.8358e-02,  ..., 2.2998e-01,\n",
      "           1.4402e-01, 2.1897e-01],\n",
      "          ...,\n",
      "          [0.0000e+00, 1.3054e-01, 0.0000e+00,  ..., 1.0779e-01,\n",
      "           9.8587e-02, 2.4621e-01],\n",
      "          [0.0000e+00, 4.8274e-02, 9.9266e-02,  ..., 1.5459e-01,\n",
      "           1.0322e-01, 1.7389e-01],\n",
      "          [0.0000e+00, 2.2023e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           2.1377e-02, 1.2306e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 2.0856e-02],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 6.9016e-02, 8.0041e-02,  ..., 2.3164e-02,\n",
      "           7.7826e-02, 2.5241e-04]],\n",
      "\n",
      "         [[1.8758e-01, 3.2473e-01, 2.1647e-01,  ..., 2.0702e-01,\n",
      "           1.1923e-01, 8.6870e-02],\n",
      "          [1.5906e-01, 1.0808e-01, 3.9516e-02,  ..., 0.0000e+00,\n",
      "           5.9665e-03, 0.0000e+00],\n",
      "          [7.0442e-02, 3.5314e-03, 2.3734e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.2310e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           8.3207e-03, 0.0000e+00],\n",
      "          [1.1256e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 5.6534e-02, 6.9069e-02,  ..., 1.1212e-01,\n",
      "           9.7612e-02, 2.0289e-01],\n",
      "          [3.2672e-02, 2.1310e-01, 1.5935e-01,  ..., 2.8662e-01,\n",
      "           1.9163e-01, 3.3967e-01],\n",
      "          [8.6156e-02, 1.7997e-01, 1.9340e-01,  ..., 2.1803e-01,\n",
      "           2.3491e-01, 3.1889e-01],\n",
      "          ...,\n",
      "          [1.1567e-01, 2.3913e-01, 1.7124e-01,  ..., 2.4505e-01,\n",
      "           2.8180e-01, 3.4749e-01],\n",
      "          [1.2681e-01, 2.5659e-01, 2.0765e-01,  ..., 2.4890e-01,\n",
      "           1.9258e-01, 3.6959e-01],\n",
      "          [0.0000e+00, 1.8135e-01, 1.1558e-01,  ..., 1.3067e-01,\n",
      "           1.0760e-01, 2.5146e-01]]]], grad_fn=<ReluBackward1>)]\n"
     ]
    }
   ],
   "source": [
    "storage = ForwardModuleStorage(cnn, [cnn.features[5], cnn.features[9]])\n",
    "a = torch.rand(1,3,100,100).to(device)\n",
    "b = torch.rand(1,3,100,100).to(device)\n",
    "storage([a, b])\n",
    "\n",
    "storage.clear()\n",
    "\n",
    "pprint(storage[cnn.features[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MutipleKeysDict([(Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
      "                  (tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00, -4.1749e-03,  4.5930e-03,  ...,  1.9435e-03,\n",
      "           -1.2495e-03, -1.1976e-04],\n",
      "          [ 0.0000e+00,  3.2739e-04,  1.0904e-02,  ..., -3.1673e-05,\n",
      "           -1.3323e-03, -3.1260e-04],\n",
      "          [ 0.0000e+00,  1.4249e-03, -5.0437e-03,  ..., -1.9025e-03,\n",
      "           -3.4659e-03, -3.3470e-03],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  2.8339e-03,  2.9887e-03,  ...,  1.8987e-03,\n",
      "            1.5888e-03,  1.4776e-03],\n",
      "          [ 0.0000e+00, -7.3478e-03, -6.1771e-03,  ...,  1.1677e-03,\n",
      "            2.3592e-05, -1.9974e-03],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           -1.3294e-03,  1.1488e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  6.7813e-04, -4.4133e-04,  ...,  7.2032e-04,\n",
      "            2.2245e-03, -1.3370e-03]],\n",
      "\n",
      "         [[-7.9905e-04,  7.1394e-04,  1.4463e-03,  ...,  2.8972e-03,\n",
      "            3.3403e-06,  1.9691e-03],\n",
      "          [-2.8441e-03, -3.4072e-03,  1.0808e-03,  ...,  1.4936e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 9.2034e-04, -2.8869e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.3411e-03,  0.0000e+00,  0.0000e+00,  ..., -1.8533e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 1.8744e-03,  0.0000e+00, -3.4953e-03,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  9.4998e-04,  ..., -2.3133e-03,\n",
      "           -1.3776e-03, -3.7028e-03],\n",
      "          [ 1.3744e-03, -1.0977e-03,  8.4094e-04,  ..., -8.0773e-04,\n",
      "           -9.2870e-04,  2.7529e-03],\n",
      "          [-3.7447e-03, -4.7988e-03, -9.9438e-05,  ..., -3.6808e-04,\n",
      "           -1.1724e-04,  4.5638e-04],\n",
      "          ...,\n",
      "          [-1.7023e-03, -2.5785e-03,  1.6681e-03,  ...,  2.0028e-03,\n",
      "           -2.0529e-03,  7.0614e-04],\n",
      "          [ 1.8094e-03, -2.5936e-03,  6.1589e-03,  ...,  5.4279e-03,\n",
      "           -3.9713e-03,  3.9225e-04],\n",
      "          [-1.3180e-03,  2.9528e-03, -1.1308e-03,  ...,  1.2964e-03,\n",
      "            1.5001e-03, -1.1318e-03]]]]),))])\n"
     ]
    }
   ],
   "source": [
    "class BackwardModuleStorage(ModuleStorage):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_hooks(how='backward')\n",
    "        \n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        super().__call__(*args, **kwargs)\n",
    "        if type(x) != list: x = [x]\n",
    "        [_x.backward() for _x in x]\n",
    "\n",
    "storage = BackwardModuleStorage({'style' : [cnn.features[5]], 'content' : [cnn.features[5], cnn.features[10]]})\n",
    "x = cnn(torch.rand(1,3,100,100).requires_grad_(True).to(device)).sum() \n",
    "storage(x, 'style')\n",
    "pprint(storage['style'])\n",
    "\n",
    "storage.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
