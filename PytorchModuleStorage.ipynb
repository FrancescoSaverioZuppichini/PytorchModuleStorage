{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial, reduce\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "from torchvision.models import vgg16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = vgg16(False).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_x = loader(content)\n",
    "# storage = StoreFeatures(cnn, [cnn[0], cnn[5]])\n",
    "# storage(content_x)\n",
    "# storage.clear()\n",
    "# print(storage._state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleStorage():\n",
    "    def __init__(self, where2layers):\n",
    "        self.where2layers = where2layers\n",
    "        self.where = list(self.names)[0]\n",
    "        self._state = OrderedDict({ k : {} for k in self.names})\n",
    "        self.unsubcribe = []\n",
    "        \n",
    "    @property\n",
    "    def names(self):\n",
    "        return self.where2layers.keys()\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        \"\"\"\n",
    "        Flat all the layers in the same array\n",
    "        \"\"\"\n",
    "        return reduce(lambda a, b: a + b, self.where2layers.values())\n",
    "    \n",
    "    def register_hooks(self, how='forward'):\n",
    "        \"\"\"\n",
    "        Loop in all the layers and register a hook. There is ONLY one hook per layer to improve\n",
    "        performance.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # create a hash of a layer as an identifier, this is unique\n",
    "            name = f\"{type(layer).__name__.lower()}-{hash(layer)}\"\n",
    "            if how == 'forward':\n",
    "                self.unsubcribe.append(layer.register_forward_hook(partial(self.hook, name=name)))\n",
    "            elif how == 'backward':\n",
    "                self.unsubcribe.append(layer.register_backward_hook(partial(self.hook, name=name)))\n",
    "            else:\n",
    "                raise ValueError(\"type must be 'forward' or 'backward'\")\n",
    "            print(f\"[INFO] {how} hook registered to {layer}\")\n",
    "        \n",
    "    def hook(self, m, i, o, name):\n",
    "        print(f\"{m} called\")\n",
    "#         store only the outputs from the correct layers defined in self.where2layers\n",
    "        if m in self.where2layers[self.where]: self._state[self.where][name] = o\n",
    "    \n",
    "    def clear(self):\n",
    "        print('[INFO] clear')\n",
    "        [un.remove() for un in self.unsubcribe]\n",
    "\n",
    "    def __call__(self, where=None):\n",
    "        if where not in self.keys(): raise(f\"we cannot find any layers with key {where}\")\n",
    "        if self.where is not None: self.where = where\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str({k: [{i : e.shape for i, e in v.items()}] for k, v in self._state.items()})    \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._state[key]\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] forward hook registered to Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[INFO] forward hook registered to Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[INFO] forward hook registered to Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "{'conv2d--9223363251159186116': tensor([[[[0.1500, 0.1389, 0.1168,  ..., 0.0892, 0.0238, 0.0000],\n",
      "          [0.1522, 0.0509, 0.0076,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1138, 0.0466, 0.0090,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1152, 0.0789, 0.0202,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1061, 0.0801, 0.0125,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0636, 0.0546, 0.0000,  ..., 0.0208, 0.0094, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0263, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0109],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0227, 0.0000, 0.0000,  ..., 0.0331, 0.0000, 0.0000],\n",
      "          [0.0049, 0.0586, 0.0456,  ..., 0.0679, 0.0790, 0.1056]],\n",
      "\n",
      "         [[0.0368, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1341, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0231, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.1477, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1320, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1164, 0.0353, 0.0149,  ..., 0.0064, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0830, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2122, 0.0895, 0.1516,  ..., 0.1033, 0.0962, 0.0189],\n",
      "          [0.1571, 0.2203, 0.1996,  ..., 0.1049, 0.0959, 0.0451],\n",
      "          ...,\n",
      "          [0.1780, 0.1954, 0.1995,  ..., 0.2111, 0.0626, 0.0000],\n",
      "          [0.1414, 0.1394, 0.0752,  ..., 0.1747, 0.1517, 0.0523],\n",
      "          [0.0706, 0.1899, 0.1875,  ..., 0.3034, 0.1816, 0.1206]],\n",
      "\n",
      "         [[0.0163, 0.0191, 0.0039,  ..., 0.1173, 0.0642, 0.1090],\n",
      "          [0.0000, 0.1710, 0.2935,  ..., 0.2619, 0.1986, 0.2505],\n",
      "          [0.0000, 0.2416, 0.2220,  ..., 0.2799, 0.2121, 0.1888],\n",
      "          ...,\n",
      "          [0.0759, 0.1853, 0.0838,  ..., 0.2113, 0.1236, 0.1674],\n",
      "          [0.0000, 0.2193, 0.2043,  ..., 0.1695, 0.2568, 0.1721],\n",
      "          [0.0000, 0.2189, 0.1892,  ..., 0.3111, 0.2024, 0.2058]],\n",
      "\n",
      "         [[0.0000, 0.0334, 0.0573,  ..., 0.0477, 0.0658, 0.0839],\n",
      "          [0.0034, 0.0426, 0.1205,  ..., 0.1325, 0.0521, 0.0150],\n",
      "          [0.0949, 0.0366, 0.0613,  ..., 0.0527, 0.0615, 0.0000],\n",
      "          ...,\n",
      "          [0.0198, 0.1043, 0.1446,  ..., 0.0733, 0.0000, 0.0145],\n",
      "          [0.0820, 0.1383, 0.0956,  ..., 0.1122, 0.0996, 0.0326],\n",
      "          [0.0259, 0.0817, 0.0678,  ..., 0.0759, 0.0249, 0.0308]]]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward1>)}\n",
      "[INFO] clear\n"
     ]
    }
   ],
   "source": [
    "class ForwardModuleStorage(ModuleStorage):\n",
    "    def __init__(self, module, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.module = module\n",
    "        self.register_hooks(how='forward')\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        super().__call__(*args, **kwargs)\n",
    "        self.module(x)\n",
    "        \n",
    "storage = ForwardModuleStorage(cnn, {'style' : [cnn.features[5]], 'content' : [cnn.features[5], cnn.features[10]]})\n",
    "storage(torch.rand(1,3,100,100).to(device), 'style')\n",
    "pprint(storage['style'])\n",
    "\n",
    "storage.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] backward hook registered to Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[INFO] backward hook registered to Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[INFO] backward hook registered to Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) called\n",
      "{'conv2d--9223363251159186116': (tensor([[[[ 7.8778e-04, -1.1037e-03,  1.3144e-03,  ..., -1.5999e-03,\n",
      "            1.3931e-03,  0.0000e+00],\n",
      "          [-1.0359e-03,  0.0000e+00,  1.0080e-02,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-2.1427e-03, -8.6761e-04, -4.3194e-03,  ...,  3.8091e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.1418e-03, -2.0311e-03,  0.0000e+00,  ..., -2.0851e-03,\n",
      "            5.1013e-04,  0.0000e+00],\n",
      "          [-1.0456e-03,  3.8216e-03,  0.0000e+00,  ..., -2.7726e-03,\n",
      "           -1.2163e-05,  0.0000e+00],\n",
      "          [ 6.6023e-04, -2.4149e-03, -3.9999e-03,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 3.6596e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.2264e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.9144e-03,  0.0000e+00,  ...,  6.7449e-04,\n",
      "            1.5436e-03,  4.7903e-04]],\n",
      "\n",
      "         [[ 2.5039e-03, -2.3166e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -1.0178e-03],\n",
      "          [ 7.3048e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 3.2769e-03,  0.0000e+00, -1.7710e-03,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.5775e-03, -4.6664e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-1.3666e-03,  0.0000e+00,  0.0000e+00,  ...,  3.2658e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 1.6241e-03, -1.5231e-03, -1.7463e-03,  ...,  6.7276e-04,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7136e-03,  0.0000e+00,  8.4675e-04,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 8.9143e-05,  1.7142e-03,  1.9503e-03,  ..., -6.2909e-05,\n",
      "           -2.2327e-03, -1.1444e-04],\n",
      "          [ 4.2394e-03,  3.0743e-03,  6.8915e-03,  ...,  7.6589e-04,\n",
      "           -1.4253e-03, -2.5286e-03],\n",
      "          ...,\n",
      "          [ 1.9336e-03, -2.1593e-03,  5.0044e-03,  ...,  3.4205e-04,\n",
      "           -8.2304e-04,  0.0000e+00],\n",
      "          [-2.0203e-03, -3.1903e-03, -2.2462e-03,  ..., -4.2634e-04,\n",
      "           -4.6482e-04,  1.0411e-03],\n",
      "          [ 2.3548e-03,  1.2813e-05,  3.4848e-04,  ..., -5.5162e-04,\n",
      "           -4.6506e-05,  2.3040e-04]],\n",
      "\n",
      "         [[-4.3335e-03,  2.1482e-03, -1.4813e-03,  ...,  2.2535e-03,\n",
      "            1.2084e-03, -1.5962e-03],\n",
      "          [ 0.0000e+00,  5.3238e-04, -1.1733e-03,  ..., -1.0514e-03,\n",
      "           -4.5098e-04, -1.2031e-03],\n",
      "          [ 0.0000e+00,  2.4923e-03,  1.6909e-03,  ...,  3.1532e-03,\n",
      "           -7.2981e-04, -3.1017e-03],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  2.2691e-04, -7.3763e-03,  ..., -1.2190e-03,\n",
      "            2.1173e-03, -3.0659e-04],\n",
      "          [ 0.0000e+00, -5.0169e-03,  2.8778e-06,  ..., -3.9429e-03,\n",
      "            1.6992e-03, -8.9604e-04],\n",
      "          [ 0.0000e+00,  7.8439e-04,  1.0369e-03,  ..., -5.8992e-04,\n",
      "            1.3080e-03,  6.0750e-04]],\n",
      "\n",
      "         [[-3.8998e-04,  2.7783e-03,  2.0453e-03,  ..., -2.0944e-03,\n",
      "            1.6469e-03,  0.0000e+00],\n",
      "          [-2.6923e-03, -2.6458e-03, -7.0144e-03,  ...,  2.2989e-03,\n",
      "            1.0150e-03,  0.0000e+00],\n",
      "          [-4.5123e-04,  4.4054e-04,  6.4295e-03,  ..., -1.4368e-03,\n",
      "            4.0715e-03, -2.4518e-05],\n",
      "          ...,\n",
      "          [-4.4553e-04,  1.0134e-03,  8.4899e-03,  ..., -3.3681e-03,\n",
      "            0.0000e+00, -1.7974e-03],\n",
      "          [-1.7188e-03, -7.5042e-04,  8.6987e-04,  ...,  3.0624e-03,\n",
      "            3.5306e-03, -4.5132e-04],\n",
      "          [-1.6829e-04,  1.2677e-03,  5.7955e-03,  ...,  2.6184e-03,\n",
      "           -2.0942e-03, -1.3269e-03]]]], device='cuda:0'),)}\n",
      "[INFO] clear\n"
     ]
    }
   ],
   "source": [
    "class BackwardModuleStorage(ModuleStorage):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_hooks(how='backward')\n",
    "        \n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        super().__call__(*args, **kwargs)\n",
    "        x.backward()\n",
    "\n",
    "storage = BackwardModuleStorage({'style' : [cnn.features[5]], 'content' : [cnn.features[5], cnn.features[10]]})\n",
    "x = cnn(torch.rand(1,3,100,100).requires_grad_(True).to(device)).sum() \n",
    "storage(x, 'style')\n",
    "pprint(storage['style'])\n",
    "\n",
    "storage.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] clear\n"
     ]
    }
   ],
   "source": [
    "storage.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
